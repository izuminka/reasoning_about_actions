{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.append('..')\n",
    "from common import *\n",
    "from analysis.model_performances import *\n",
    "from questions_construction.questions import *\n",
    "from copy import deepcopy\n",
    "from helpers import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "TO_PRETTY = {'pos': 'Positive Fluents', 'neg': 'Negative Fluents', 'pos_neg': 'Pos. and Neg. Fluents'}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T21:29:44.540320Z",
     "start_time": "2024-11-22T21:29:34.662004Z"
    }
   },
   "id": "1581e9208ff09f8e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:29:47.287076Z",
     "start_time": "2024-11-22T21:29:45.972013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions_by_id = {d[OUT_OBJ_ID]: d for d in open_jsonl(f'{DATA_PATH}/test_data.paraphrased.cleaned.jsonl')}\n",
    "override = True\n",
    "\n",
    "answer_type = TRUE_FALSE_ANSWER_TYPE\n",
    "answer_response_type = f'{answer_type}.{ACCURACY_SCORE_KEY}' #f'{TRUE_FALSE_ANSWER_TYPE}.{ACCURACY_SCORE_KEY}' #\n",
    "stats_save_dir = f'{STATISTICS_PATH}.trial_run.ED'\n",
    "\n",
    "\n",
    "substitution = WITHOUT_RANDOM_SUB\n",
    "ramification = WITHOUT_RAMIFICATIONS\n",
    "prompt_type = ZERO_SHOT_PROMPT_KEY #FEW_SHOT_3_PROMPT_KEY #\n",
    "models = ['gpt-4o', 'llama_8b','llama_70b', 'llama_8b.finetuned_tf']\n",
    "\n",
    "data_all = []\n",
    "for model_name in models:\n",
    "    model_results_dir = f'{CODE_PATH}/results/without_ramifications/{prompt_type}/{model_name}.jsonl'\n",
    "    model_results = open_jsonl(model_results_dir)\n",
    "    data_all.extend(model_results)\n",
    "\n",
    "\n",
    "data_all_clean = []\n",
    "for d in data_all:\n",
    "    if d[IS_RESPONSE_CORRECT_KEY] in (TRUE_ANSWER, FALSE_ANSWER):\n",
    "        data_all_clean.append(d)\n",
    "data_all = data_all_clean\n",
    "\n",
    "\n",
    "def tmp(stats):\n",
    "        res_obj = stats.compute()\n",
    "        if res_obj:\n",
    "            mean = res_obj['result']\n",
    "            sem = None\n",
    "            if res_obj['result_other']:\n",
    "                sem = res_obj['result_other']['sem']\n",
    "            final_res = (mean, sem)\n",
    "        else:\n",
    "            final_res = (None, None)\n",
    "        final_res = tuple([round(v*100, 2) if v else v for v in final_res ])\n",
    "        return'${'+str(final_res[0])+'}_{'+str(final_res[1])+'}$'\n",
    "    "
   ],
   "id": "ff4516a101f58a11",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8202e32f1e91d8a0"
  },
  {
   "cell_type": "code",
   "source": [
    "def to_df_by_models(data_all, prompt_type,models, ramifications, subs, plan_length,\n",
    "                      score_type=ACCURACY_SCORE_KEY, \n",
    "                      question_category = ALL_QUESTION_CATEGORIES_KEY,\n",
    "                      domain = ALL_DOMAINS_KEY,\n",
    "                      answer_type=TRUE_FALSE_ANSWER_TYPE):\n",
    "\n",
    "    # index = []\n",
    "    data_for_df = []    \n",
    "    for fluent_type in FLUENT_TYPES_LIST:\n",
    "        # index.append()\n",
    "        data_columns = {}\n",
    "        data_columns['fluent type'] = TO_PRETTY.get(fluent_type, fluent_type)\n",
    "        for model in models:\n",
    "            data = filter_multi_selector_modified(data_all, ramifications, model, prompt_type, answer_type, subs, plan_length, [(OUT_OBJ_FLUENT_TYPE, {fluent_type})])\n",
    "            stats = TrueFalseStatsCustom(data, plan_length, question_category, ramifications, model, prompt_type, domain, subs, score_type=score_type)\n",
    "            res_obj = stats.compute()\n",
    "            if res_obj:\n",
    "                mean = res_obj['result']\n",
    "                sem = None\n",
    "                if res_obj['result_other']:\n",
    "                    sem = res_obj['result_other']['sem']\n",
    "                final_res = (mean, sem)\n",
    "            else:\n",
    "                final_res = (None, None)\n",
    "            final_res = tuple([round(v*100, 2) if v else v for v in final_res ])\n",
    "            final_res = '${'+str(final_res[0])+'}_{'+str(final_res[1])+'}$'\n",
    "            # print(final_res)\n",
    "            data_columns[TO_PRETTY.get(model, model)] = final_res\n",
    "        data_for_df.append(data_columns)\n",
    "    return pd.DataFrame(data_for_df)\n",
    "\n",
    "def to_df_by_models_sign(data_all, prompt_type,models, ramifications, subs, plan_length,\n",
    "                      score_type=ACCURACY_SCORE_KEY, \n",
    "                      question_category = ALL_QUESTION_CATEGORIES_KEY,\n",
    "                      domain = ALL_DOMAINS_KEY,\n",
    "                      answer_type=TRUE_FALSE_ANSWER_TYPE):\n",
    "\n",
    "    # index = []\n",
    "    data_for_df = []    \n",
    "    for fluent_sign in POS_NEG_FLUENTS_KEY_LIST:\n",
    "        # index.append()\n",
    "        data_columns = {}\n",
    "        data_columns['fluent sign'] = TO_PRETTY[fluent_sign] \n",
    "        for model in models:\n",
    "            data = filter_multi_selector_modified(data_all, ramifications, model, prompt_type, answer_type, subs, plan_length, [(OUT_OBJ_FLUENT_SIGN_QUESTION, {fluent_sign})])\n",
    "            stats = TrueFalseStatsCustom(data, plan_length, question_category, ramifications, model, prompt_type, domain, subs, score_type=score_type)\n",
    "            res_obj = stats.compute()\n",
    "            if res_obj:\n",
    "                mean = res_obj['result']\n",
    "                sem = None\n",
    "                if res_obj['result_other']:\n",
    "                    sem = res_obj['result_other']['sem']\n",
    "                final_res = (mean, sem)\n",
    "            else:\n",
    "                final_res = (None, None)\n",
    "            final_res = tuple([round(v*100, 2) if v else v for v in final_res ])\n",
    "            final_res = '${'+str(final_res[0])+'}_{'+str(final_res[1])+'}$'\n",
    "            # print(final_res)\n",
    "            data_columns[TO_PRETTY.get(model, model)] = final_res\n",
    "        data_for_df.append(data_columns)\n",
    "    return pd.DataFrame(data_for_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T21:30:33.731726Z",
     "start_time": "2024-11-22T21:30:33.716844Z"
    }
   },
   "id": "dd77fda858739d0e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T21:30:34.536500Z",
     "start_time": "2024-11-22T21:30:34.211073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plan_length = 19\n",
    "df = to_df_by_models(data_all, prompt_type, models, ramification, substitution,plan_length)\n",
    "# df = to_df_by_models_sign(data_all, prompt_type, models, ramification, substitution,plan_length)\n",
    "df\n",
    "        \n",
    "# caption_nl = f'performance of {model_name}, {prompt_type}, pl-{plan_length}'.replace('_', ' ')\n",
    "# save_key = f'{model_name}.{prompt_type}.{plan_length}'\n",
    "# \n",
    "# latex_table_all = to_latex_table(df, caption_nl, label=save_key, index=False)\n",
    "# with open(os.path.join(save_dir, f'{save_key}.tex'), 'w') as f:\n",
    "#     f.write(latex_table_all)"
   ],
   "id": "da82c99c16ce0495",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          fluent type            gpt-4o          llama_8b         llama_70b  \\\n",
       "0        base_fluents  ${90.24}_{4.63}$  ${63.64}_{8.37}$  ${76.19}_{6.57}$   \n",
       "1     derived_fluents   ${74.42}_{4.7}$  ${44.07}_{6.46}$  ${67.47}_{5.14}$   \n",
       "2  persistent_fluents   ${80.0}_{4.34}$  ${70.91}_{6.12}$  ${71.74}_{4.69}$   \n",
       "3      static_fluents  ${82.76}_{7.01}$  ${60.71}_{9.23}$   ${67.5}_{7.41}$   \n",
       "\n",
       "  llama_8b.finetuned_tf  \n",
       "0      ${83.61}_{4.74}$  \n",
       "1       ${76.47}_{4.2}$  \n",
       "2      ${93.46}_{2.39}$  \n",
       "3      ${96.88}_{2.17}$  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fluent type</th>\n",
       "      <th>gpt-4o</th>\n",
       "      <th>llama_8b</th>\n",
       "      <th>llama_70b</th>\n",
       "      <th>llama_8b.finetuned_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_fluents</td>\n",
       "      <td>${90.24}_{4.63}$</td>\n",
       "      <td>${63.64}_{8.37}$</td>\n",
       "      <td>${76.19}_{6.57}$</td>\n",
       "      <td>${83.61}_{4.74}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>derived_fluents</td>\n",
       "      <td>${74.42}_{4.7}$</td>\n",
       "      <td>${44.07}_{6.46}$</td>\n",
       "      <td>${67.47}_{5.14}$</td>\n",
       "      <td>${76.47}_{4.2}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>persistent_fluents</td>\n",
       "      <td>${80.0}_{4.34}$</td>\n",
       "      <td>${70.91}_{6.12}$</td>\n",
       "      <td>${71.74}_{4.69}$</td>\n",
       "      <td>${93.46}_{2.39}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>static_fluents</td>\n",
       "      <td>${82.76}_{7.01}$</td>\n",
       "      <td>${60.71}_{9.23}$</td>\n",
       "      <td>${67.5}_{7.41}$</td>\n",
       "      <td>${96.88}_{2.17}$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T21:30:35.270445Z",
     "start_time": "2024-11-22T21:30:35.104139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latex_table_all = to_latex_table(df, 'some caption', index=False)\n",
    "print(latex_table_all)"
   ],
   "id": "b6b0cbdd30bc9a19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}[h!]\n",
      "\\begin{adjustbox}{width=\\textwidth,center}\n",
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "       fluent type &           gpt-4o &         llama_8b &        llama_70b & llama_8b.finetuned_tf \\\\\n",
      "\\midrule\n",
      "      base_fluents & ${90.24}_{4.63}$ & ${63.64}_{8.37}$ & ${76.19}_{6.57}$ &      ${83.61}_{4.74}$ \\\\\n",
      "   derived_fluents &  ${74.42}_{4.7}$ & ${44.07}_{6.46}$ & ${67.47}_{5.14}$ &       ${76.47}_{4.2}$ \\\\\n",
      "persistent_fluents &  ${80.0}_{4.34}$ & ${70.91}_{6.12}$ & ${71.74}_{4.69}$ &      ${93.46}_{2.39}$ \\\\\n",
      "    static_fluents & ${82.76}_{7.01}$ & ${60.71}_{9.23}$ &  ${67.5}_{7.41}$ &      ${96.88}_{2.17}$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\end{adjustbox}\n",
      "\\caption{some caption}\n",
      "\\label{table:}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paveldolin/dev/research/completed/reasoning_about_actions/pipeline/src/analysis/tables/helpers.py:45: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex_table = df.to_latex(index=index, formatters={\"name\": str.upper}, float_format=\"{:.2f}\".format)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "df_pos_neg = to_df_by_sub_ram_neg_pos(data_all, model_name, prompt_type, plan_length=plan_length)\n",
    "df_pos_neg\n",
    "        \n",
    "# caption_nl = f'performance of {model_name}, {prompt_type}, pl-{plan_length}'.replace('_', ' ')\n",
    "# save_key = f'{model_name}.{prompt_type}.{plan_length}'\n",
    "# \n",
    "# latex_table_all = to_latex_table(df, caption_nl, label=save_key, index=False)\n",
    "# with open(os.path.join(save_dir, f'{save_key}.tex'), 'w') as f:\n",
    "#     f.write(latex_table_all)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T02:25:46.704588Z",
     "start_time": "2024-11-21T02:25:46.666764Z"
    }
   },
   "id": "a2a6c92185f6a2e3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUT_OBJ_IS_POS_FLUENT_QUESTION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [146]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_pos_neg \u001B[38;5;241m=\u001B[39m \u001B[43mto_df_by_sub_ram_neg_pos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_all\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplan_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mplan_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m df_pos_neg\n",
      "Input \u001B[0;32mIn [78]\u001B[0m, in \u001B[0;36mto_df_by_sub_ram_neg_pos\u001B[0;34m(data_all, model_name, prompt_type, score_type, question_category, domain, answer_type, plan_length)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subs \u001B[38;5;129;01min\u001B[39;00m [WITHOUT_RANDOM_SUB, WITH_RANDOM_SUB]:\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ramifications \u001B[38;5;129;01min\u001B[39;00m [WITHOUT_RAMIFICATIONS, WITH_RAMIFICATIONS]:\n\u001B[0;32m---> 16\u001B[0m         data \u001B[38;5;241m=\u001B[39m filter_multi_selector_modified(data_all, ramifications, model_name, prompt_type, answer_type, subs, plan_length, [(\u001B[43mOUT_OBJ_IS_POS_FLUENT_QUESTION\u001B[49m, {is_pos_fluent})])\n\u001B[1;32m     17\u001B[0m         stats \u001B[38;5;241m=\u001B[39m TrueFalseStatsCustom(data, plan_length, question_category, ramifications, model_name, prompt_type, domain, subs, score_type\u001B[38;5;241m=\u001B[39mscore_type)\n\u001B[1;32m     18\u001B[0m         res_obj \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mcompute()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'OUT_OBJ_IS_POS_FLUENT_QUESTION' is not defined"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "cell_type": "markdown",
   "source": [
    "# By Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9896e750372f819e"
  },
  {
   "cell_type": "code",
   "source": [
    "score_type=ACCURACY_SCORE_KEY, \n",
    "question_category = ALL_QUESTION_CATEGORIES_KEY,\n",
    "domain = ALL_DOMAINS_KEY,\n",
    "answer_type=TRUE_FALSE_ANSWER_TYPE,\n",
    "plan_length=1\n",
    "\n",
    "data_for_df = []    \n",
    "for fluent_type in FLUENT_TYPES_LIST:\n",
    "    # index.append()\n",
    "    data_columns = {}\n",
    "    data_columns['fluent type'] = TO_PRETTY[fluent_type]\n",
    "    for model in models:\n",
    "        data = filter_multi_selector_modified(data_all, ramification, model, prompt_type, answer_type, substitution, plan_length, [(OUT_OBJ_FLUENT_TYPE, {fluent_type})])\n",
    "        print(len(data))\n",
    "        stats = TrueFalseStatsCustom(data, plan_length, question_category, ramification, model, prompt_type, domain, substitution, score_type=score_type)\n",
    "        res_obj = stats.compute()\n",
    "        if res_obj:\n",
    "            mean = res_obj['result']\n",
    "            sem = None\n",
    "            if res_obj['result_other']:\n",
    "                sem = res_obj['result_other']['sem']\n",
    "            final_res = (mean, sem)\n",
    "        else:\n",
    "            final_res = (None, None)\n",
    "        final_res = tuple([round(v*100, 2) if v else v for v in final_res ])\n",
    "        final_res = '${'+str(final_res[0])+'}_{'+str(final_res[1])+'}$'\n",
    "        # print(final_res)\n",
    "        data_columns[TO_PRETTY.get(model, model)] = final_res\n",
    "    data_for_df.append(data_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T06:40:44.763603Z",
     "start_time": "2024-11-20T06:40:44.670001Z"
    }
   },
   "id": "f5a86ce0268bbed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T06:45:05.320630Z",
     "start_time": "2024-11-20T06:45:05.302489Z"
    }
   },
   "cell_type": "code",
   "source": "data = filter_multi_selector_modified(data_all, ramification, 'llama_8b', prompt_type, 'free_answer', substitution, 1, [(OUT_OBJ_FLUENT_TYPE, {fluent_type})])",
   "id": "4f5eb98e94bbf763",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T06:45:06.605467Z",
     "start_time": "2024-11-20T06:45:06.599796Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "d5a56939d3a52596",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T06:33:06.946940Z",
     "start_time": "2024-11-20T06:33:06.941696Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_type",
   "id": "b914aaf0385d3bcf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero_shot'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T06:41:41.258452Z",
     "start_time": "2024-11-20T06:41:41.254560Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_type",
   "id": "6d8534fe3b93dad2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero_shot'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T06:41:50.847571Z",
     "start_time": "2024-11-20T06:41:50.842945Z"
    }
   },
   "cell_type": "code",
   "source": "substitution",
   "id": "b96ac014cf15e22",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'without_random_sub'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T06:42:01.828974Z",
     "start_time": "2024-11-20T06:42:01.823347Z"
    }
   },
   "cell_type": "code",
   "source": "data_all[0]",
   "id": "c28b3db37fcf6ef5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': '784689cc-3f7c-4a22-824a-ffc0e4c90483',\n",
       " 'domain_name': 'logistics',\n",
       " 'instance_id': 'Instance_2',\n",
       " 'question_category': 'composite',\n",
       " 'question_name': 'iter_6_question_2',\n",
       " 'fluent_type': 'all_fluents',\n",
       " 'answer_type': 'free_answer',\n",
       " 'question': 'Given the initial condition, the following actions are planned to be performed: at airport l1_1, package p2 is loaded in truck t1, at airport l1_1, package p1 is loaded in truck t1, truck t1 is driven to airport l1_0 from airport l1_1 in city c1, in city c1, truck t0 is driven from airports l1_1 to l0_1, from truck t1 package p1 is unloaded at airport l1_0, in city c0, truck t0 is driven from airports l0_1 to l0_0, at airport l0_0, package p3 is loaded in airplane a0, airplane a0 flies from airports l0_0 to l1_0, from airplane a0 package p3 is unloaded at airport l1_0 and package p3 is loaded in truck t1 at airport l1_0 to reach the current state. What are the valid properties of the state that do not involve negations for p1 before the first infeasible action in the sequence? Write None if there are none',\n",
       " 'answer': 'package p1 is in vehicle t1',\n",
       " 'plan_length': 10,\n",
       " 'initial_state_nl': 'Airport l0_0 is in city c0, airport l0_1 is located in city c0, airport l1_0 is in city c1, at airport l0_0, object a0 is located, at airport l0_0, object p3 is located, at airport l0_1, object t0 is located, at airport l1_1, object t1 is located, city c1 contains airport l1_1, object p0 is located at airport l0_1, object p1 is at airport l1_1 and object p2 is at airport l1_1.',\n",
       " 'fluent_sign_question': 'pos_neg',\n",
       " 'question_subcategories': ['fluent_tracking',\n",
       "  'action_executability',\n",
       "  'object_tracking'],\n",
       " 'question_paraphrased': 'Given the initial condition, the following actions are scheduled to take place: at airport l1_1, package p2 is loaded onto truck t1, followed by the loading of package p1 onto truck t1 at the same airport, then truck t1 is driven from airport l1_1 to airport l1_0 in city c1, while in city c1, truck t0 is driven from airports l1_1 to l0_1, subsequently, package p1 is unloaded from truck t1 at airport l1_0, in city c0, truck t0 is driven from airports l0_1 to l0_0, then at airport l0_0, package p3 is loaded onto airplane a0, airplane a0 flies from airports l0_0 to l1_0, and finally, package p3 is unloaded from airplane a0 at airport l1_0 and loaded onto truck t1 at the same airport to reach the current state. What are the valid properties of the state that do not involve negations for p1 before the first infeasible action in the sequence? Write None if there are none.',\n",
       " 'initial_state_nl_paraphrased': 'Airport l0_0 is situated in city c0, while airport l0_1 is also located in city c0. Airport l1_0 is found in city c1. At airport l0_0, you can find object a0, and object p3 is also located there. Airport l0_1 is home to object t0, whereas airport l1_1 has object t1. City c1 encompasses airport l1_1. Additionally, object p0 is situated at airport l0_1, and both object p1 and object p2 are located at airport l1_1.',\n",
       " 'response': '[REASON]\\nBased on the initial conditions and the given sequence of actions, we can analyze the state of objects and locations at each step. We are looking for valid properties that do not involve negations for object p1 before the first infeasible action.\\n\\n1. Initially, p1 is at airport l1_1.\\n2. Then, p2 is loaded onto truck t1 at airport l1_1, but this does not affect p1.\\n3. Next, p1 is loaded onto truck t1 at airport l1_1, so p1 is now inside truck t1 at airport l1_1.\\n4. After that, truck t1 is driven from airport l1_1 to airport l1_0 in city c1. This means p1 is inside truck t1 at airport l1_0.\\n5. Then, truck t0 is driven from airport l1_1 to airport l0_1 in city c1, but this does not affect p1.\\n6. Next, p1 is unloaded from truck t1 at airport l1_0. This means p1 is no longer inside truck t1 but is at airport l1_0.\\n7. After that, truck t0 is driven from airport l0_1 to airport l0_0 in city c0, but this does not affect p1.\\n8. Finally, package p3 is loaded onto airplane a0 at airport l0_0, and then airplane a0 flies from airport l0_0 to airport l1_0. This action is the first infeasible action in the sequence because p3 is already at airport l1_0.\\n\\n[ANSWER]\\np1 is at airport l1_0.',\n",
       " 'response_clean': 'p1 is at airport l1_0.',\n",
       " 'evaluated_free_answer_response': '[Reasoning for the evaluation]\\nThe LLM response states that p1 is at airport l1_0, but the Ground Truth states that package p1 is in vehicle t1. These are two different locations, and there is no information in the Ground Truth that indicates the location of vehicle t1.\\n\\n[Answer]\\nFalse',\n",
       " 'model': 'llama_8b',\n",
       " 'prompt_type': 'zero_shot',\n",
       " 'ramification_type': 'without_ramifications',\n",
       " 'substitution_type': 'without_random_sub',\n",
       " 'unique_id': '784689cc-3f7c-4a22-824a-ffc0e4c90483::llama_8b::zero_shot::without_ramifications::without_random_sub'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53bcf9d8cd36d49c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
